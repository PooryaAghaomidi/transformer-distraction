{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdfc11cba5879833"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_value= 42\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "490746e66a239098"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d652632d4bcd1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fbd03183481a592"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from livelossplot.inputs.tf_keras import PlotLossesCallback"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10d4ed20a7a908c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:01:42.231318800Z",
     "start_time": "2023-12-10T12:01:42.215351900Z"
    }
   },
   "id": "754086504300585b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs  = 20\n",
    "batch_size  = 2\n",
    "cls_num     = 2\n",
    "shape       = (40, 200, 150, 1)\n",
    "lr          = 0.01\n",
    "wd          = 1e-5\n",
    "opt         = optimizers.SGD(learning_rate=lr)\n",
    "los         = losses.CategoricalCrossentropy()\n",
    "mtr         = 'accuracy'\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (shape[0] // PATCH_SIZE[0]) ** 2\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872d85dd30d2dff0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "address         = 'videos lts/videos lts/'\n",
    "files           = os.listdir(address)\n",
    "\n",
    "l = len(files)\n",
    "random.Random(seed_value).shuffle(files)\n",
    "\n",
    "X_train = files[:int(0.8 * l)]\n",
    "X_test = files[int(0.8 * l):int(0.9 * l)]\n",
    "X_val = files[int(0.9 * l):]\n",
    "\n",
    "steps_per_train = len(X_train) // batch_size\n",
    "steps_per_test  = len(X_test) // batch_size\n",
    "steps_per_val   = len(X_val) // batch_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a5d948a8969912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ad199163e669ffd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, shape, batch_size, cls_num):\n",
    "        self.data       = data\n",
    "        self.shape      = shape\n",
    "        self.batch_size = batch_size\n",
    "        self.cls_num    = cls_num\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes       = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.data[k] for k in indexes]\n",
    "        x, y          = self.__data_generation(list_IDs_temp)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        x = np.empty((self.batch_size, int(self.shape[0]), int(self.shape[1]), int(self.shape[2]), int(self.shape[3])))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            y[i]  = int(ID[-3])\n",
    "            for img in range(int(self.shape[0])):\n",
    "                my_img = image.img_to_array(image.load_img('videos lts/videos lts/' + ID + '/' + str(img) + '.jpg', color_mode='grayscale', target_size=(int(self.shape[1]), int(self.shape[2]))))\n",
    "                my_img /= 255.0\n",
    "                x[i, img, :, :, :] = my_img\n",
    "\n",
    "        return x, to_categorical(y, num_classes=self.cls_num)\n",
    "\n",
    "\n",
    "train_gen = DataGenerator(X_train, shape, batch_size, cls_num)\n",
    "test_gen  = DataGenerator(X_test , shape, batch_size, cls_num)\n",
    "val_gen   = DataGenerator(X_val  , shape, batch_size, cls_num)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a41b6e143a0789d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T11:47:42.187593500Z",
     "start_time": "2023-12-10T11:47:42.177419900Z"
    }
   },
   "id": "8b75fb53b26180fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n",
    "    \n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "177bbe50dff34117"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=shape,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=cls_num,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8369543e4b538a6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "  model = create_vivit_classifier(tubelet_embedder=TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE),\n",
    "                                  positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM))\n",
    "\n",
    "  return model\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer=opt, loss=los, metrics=mtr)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0350cf0b5c877f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:03:47.758501300Z",
     "start_time": "2023-12-10T15:03:47.739621100Z"
    }
   },
   "id": "424ed491ca413edf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def callback():\n",
    "  mymonitor = 'val_loss'\n",
    "  mymode    = 'min'\n",
    "\n",
    "  main_chk  = ModelCheckpoint(filepath='my_checkpoint', monitor=mymonitor, mode=mymode, verbose=1, save_best_only=True)\n",
    "  early_st  = EarlyStopping(monitor=mymonitor, mode=mymode, patience=30, verbose=1)\n",
    "  rduce_lr  = ReduceLROnPlateau(monitor=mymonitor, mode=mymode, factor=0.5, patience=5, verbose=1, min_lr=0.0001)\n",
    "  tr_plot   = PlotLossesCallback()\n",
    "\n",
    "  return [main_chk, rduce_lr, tr_plot]\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callback(),\n",
    "                    steps_per_epoch  = steps_per_train,\n",
    "                    validation_steps = steps_per_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af159589ea12ec79"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T21:22:43.952192300Z",
     "start_time": "2023-12-10T21:22:43.726461200Z"
    }
   },
   "id": "fa43565d3dbb6d60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T21:25:19.988890900Z"
    }
   },
   "id": "459cd133772d7932"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testmodel = load_model('my_checkpoint', compile=True)\n",
    "tst_loss , tst_acc = testmodel.evaluate(test_gen, steps = steps_per_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0f3ef8972779a60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
